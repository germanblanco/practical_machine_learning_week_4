---
title: "Physical Exercise Quality Prediction"
author: "German Blanco"
date: "8 de noviembre de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(7777)
library(caret)
library(ggplot2)
```

## Physical Exercise Quality Prediction

This is the Peer-graded Assignment exercise of week 4 (the last week) of Coursera course "Practical Machine Learning". The goal is to predict quality of physical exercise on the Weight Lifting Exercise Dataset described in [this page](http:/groupware.les.inf.puc-rio.br/har).

### Problem analysis

```{r load_train_csv}
train_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
test_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

In the next step, both the training and test CSV files are analyzed, not in order to shape the prediction algorithm, but in order to know what is it the exercise is about.

### Observations on the exercise objectives:

1 - The training dataset contains series of samples, rather than individual instances. Each series of samples seems to be identified by a "user_name" and a "sequence_number". All series of samples provided have the same value of "classe", which indicates that it is the series of samples that is classified, instead of the individual instances. There seems to be an opportunity of aggregating the data of a series of samples in a meaningful way in order to produce a classification of the complete series.

2 - On the other hand, the testing dataset contains only individual instances of the series. There are data points in the training set of the same series as those in the test dataset. It seems that the goal to predict the "classe" of each of this intances in the test set, without any interpretation of what the data is about, may be achieved by looking at the "classe" of the series of the test datapoints in the training set. However, reading what the experiment is about, it seems that the question should be about detecting the quality of the exercise by looking at the data obtained with the sensors. It would seem that an aggregation of the data of each of the series could provide more information to do this, but in the test set only one instance of data point per series is provided, and that seems to indicate that the goal is to identify the quality of the exercise by looking at the sensor data of just one data point of the series.

3 - The test data set contains the same variables as the training dataset, however many of these variables have no valid values at all in the dataset. Since they are not going to be valuable for the prediction of the values of the dataset, they will not be considered for the analysis.

4 - Since the data is grouped in series of measurements, there could be a strong correlation between elements in the same series. Unless proper care is taken, the prediction might tend to be more about associating datapoints with an existing series than with correctly cathegorizing a data point in one of the 6 classes by itself, independently of which series it was included in.


### Cleaning up of data

According to the previous observations:

- There will be no aggregation of data in the series of observations.

- Columns with no valid values in the test set will be filtered out.

- Only sensor data will be considered.

- Complete series will be used when splitting the data in order to verify the accuracy of the models.

```{r clean_up_data}
## Verify that all datapoints in a series have the same classe
series_classe <- aggregate(classe ~ user_name + num_window, data=train_csv_df, FUN=function(x) { paste(unique(x)) })
## Samples for each user and classe will be needed in each split
series_classe$to_split <- as.factor(paste(series_classe$user_name, series_classe$classe))
## function that returns the classe for each datapoint and was used to obtain the answers to the quize with 100% accuracy
get_classe <- function(x) { series_classe[series_classe$user_name == x$user_name & series_classe$num_window == x$num_window,] }
## Example "prediction" on the test set
get_classe(test_csv_df[10,])
## Get all columns that have all NA values from test set
cols_all_na <- sapply(test_csv_df, function(x)all(is.na(x)))
## Select only sensor data columns with valid values in test set from training set
train_valid_sensor_data_df <- train_csv_df[,!cols_all_na][,seq(8,59)]
## Add the variable that is to be predicted
train_valid_sensor_data_df$classe <- train_csv_df$classe
## There are no NA values now in the dataframe
length(train_valid_sensor_data_df$roll_belt) == length(complete.cases(train_valid_sensor_data_df))
```

### Data Slicing

There are now 52 columns of numeric data in the training dataset. The next step is to slice out a subset of our training set in order to verify the prediction algorithm. In order to verify if the prediction algorithm really cathegorizes datapoints by themselves in one of the existing 5 classes, the split will be based on "num_window". This will put only complete series in both the training set and the data set. There will be two splits in order to combine different algorithms in an ensemble later.`

```{r slice_out_verify}
## Slice out aprox. 25% of data of the total training dataset
splits <- createDataPartition(y=series_classe$to_split, times=2, p=0.75, list=FALSE)
inTrainAll <- splits[,1]
## Slice out aprox. 25% of data again to set aside for training the ensemble
inTrain <- splits[,2]
## Recover series identification information
train_valid_sensor_data_df$num_window <- train_csv_df$num_window
## Data for final verification of the ensemble
inVerify <- train_valid_sensor_data_df$num_window %in% series_classe[-inTrainAll,]$num_window
## Data for training the models
inTrainModels <- !inVerify & (train_valid_sensor_data_df$num_window %in% series_classe[inTrain,]$num_window)
## Data for training the ensemble of models
inTrainEnsemble <- !inVerify & !inTrainModels
## Datasets for each of the 3 partitions
my_verify_df_with_num_window <- train_valid_sensor_data_df[inVerify,]
my_train_ensemble_df_with_num_window <- train_valid_sensor_data_df[inTrainEnsemble,]
my_train_df_with_num_window <- train_valid_sensor_data_df[inTrainModels,]
## Remove the num_window column now
my_train_df <- my_train_df_with_num_window[,-length(colnames(my_train_df_with_num_window))]
my_train_ensemble_df <- my_train_ensemble_df_with_num_window[,-length(colnames(my_train_df_with_num_window))]
my_verify_df <- my_verify_df_with_num_window[,-length(colnames(my_verify_df_with_num_window))]
```

### Preprocessing the Data

There are 52 columns of numeric data in the training dataset now. In order to help convergence of algorithms, and in order to be able to compare the relevance of parameters, normalization and PCA will be applied.

```{r preprocessing_data}
## Preprocessing of data, using all columns except for the last one
preNormalization <- preProcess(my_train_df[,-length(colnames(my_train_df))], method=c("center", "scale"))
my_train_normalized_df <- predict(preNormalization, my_train_df[,-length(colnames(my_train_df))])
## Principal Component Analysis using caret
prePCA <- prcomp(my_train_normalized_df)
```

The following figure shows the percentage of variance explained by each of the principal components. The first 20 principal components explain more than 90% of the variance in the dataset, so there is a lot of redundancy.

```{r variance_explained}
std_dev <- prePCA$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex,  type="b")
# PCA using caret and keeping only 20 components
prePCA_caret <- preProcess(my_train_normalized_df, method="pca", pcaComp=20)
trainPC <- predict(prePCA_caret, my_train_normalized_df)
```

### Generalization for Different Users

At this point, since we have six different users in the system, I would like to know if the data under analysis is similar from one user to the other. Even though only datapoints from this six different are present in the test dataset, it seems interesting to know if the prediction should be done in a user per user basis or can be applied to data coming from any user. The following figure shows a clear difference in the data collected from each of the users. This would mean that (at least until data from more users is present) it would make sense to produce predictions per user. "user_name" will be added to the Principal Components as input to the models, and it will not be reliable to predict the "classe" for datapoints that have a "user_name" that was not included in the training set. Fortunately, this does not occur in our current test data.

```{r relationship_with_user_name}
train_data_raw <- my_train_df
train_data_raw$user_name <- train_csv_df[inTrainModels,]$user_name
train_data_raw$classe <- train_csv_df[inTrainModels,]$classe
dummy <- dummyVars(~ user_name, train_csv_df[inTrainModels,])
train_data_norm <- cbind(trainPC, predict(dummy, train_csv_df[inTrainModels,]))
train_data_norm$classe <- train_csv_df[inTrainModels,]$classe
```

```{r plot_1, echo=FALSE}
qplot(train_data_raw$user_name, train_data_norm$PC1, fill=train_data_norm$classe, geom=c("boxplot"))
```

### Poll Ensemble

The goal is to train a ensemble of a Random Forest, a K Nearest Neighbors and a Naive Bayes classifiers.

```{r poll_ensemble}
## Since these are computationally expensive, set up parallel processing
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
## Predictions on the training set for the ensemble
train_ensemble_data_raw <- my_train_ensemble_df
train_ensemble_data_raw$user_name <- train_csv_df[inTrainEnsemble,]$user_name
train_ensemble_data_raw$classe <- train_csv_df[inTrainEnsemble,]$classe
prePCA_ensemble <- prcomp(my_train_normalized_df)
my_train_ensemble_normalized_df <- predict(preNormalization, my_train_ensemble_df[,-length(colnames(my_train_ensemble_df))])
trainEnsemblePC <- predict(prePCA_caret, my_train_ensemble_normalized_df)
train_ensemble_data_norm <- cbind(trainEnsemblePC, predict(dummy, train_csv_df[inTrainEnsemble,]))
train_ensemble_data_norm$classe <- train_csv_df[inTrainEnsemble,]$classe
rf_model <- train(classe ~ ., data=train_data_raw, method='rf')
knn_model <- train(classe ~ ., data=train_data_norm, method='knn')
svm_model <- train(classe ~ ., data=train_data_raw, method='svmRadial')
## Stop the cluster now that the training is  done
stopCluster(cl)
## Obtain the predictions
train_ensemble_data <- data.frame(classe=train_csv_df[inTrainEnsemble,]$classe)
train_ensemble_data$prediction_rf <- predict(rf_model, train_ensemble_data_raw)
train_ensemble_data$prediction_knn <- predict(knn_model, train_ensemble_data_norm)
train_ensemble_data$prediction_svm <- predict(svm_model, train_ensemble_data_raw)
## Measure accuracy for each of the models
confusionMatrix(train_ensemble_data$prediction_rf, train_ensemble_data$classe)
confusionMatrix(train_ensemble_data$prediction_knn, train_ensemble_data$classe)
confusionMatrix(train_ensemble_data$prediction_svm, train_ensemble_data$classe)
## Build a new model that combines the predictions
classe_levels <- levels(train_ensemble_data$classe)
train_ensemble_data$prediction_ensemble_str <- ifelse(
  train_ensemble_data$prediction_svm == train_ensemble_data$prediction_knn,
  classe_levels[train_ensemble_data$prediction_svm],
  classe_levels[train_ensemble_data$prediction_rf])
train_ensemble_data$prediction_ensemble <- as.factor(train_ensemble_data$prediction_ensemble_str)
## Measure accuracy for the ensemble
confusionMatrix(train_ensemble_data$prediction_ensemble, train_ensemble_data$classe)
```

### Results

The accuracy in the verification dataset of the best classifier obtained is shown in the following confusion matrix. The ensemble did not work well in this case, it seems that SVM and KNN agreed in the wrong result quite often, so Random Forest is the best alternative.

```{r estimated_accuracy}
confusionMatrix(train_ensemble_data$prediction_rf, train_ensemble_data$classe)
```

It is even possible to check the predictions in the test dataset, since there is a function that obtains the class of the corresponding data series.

```{r accuracy_on_test_set}
test_prediction <- predict(rf_model, test_csv_df)
## Only one error!
for (idx in 1:20) { print(get_classe(test_csv_df[idx,])$classe == as.character(test_prediction[idx])) }
```
