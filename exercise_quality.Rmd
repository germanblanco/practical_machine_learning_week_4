---
title: "Physical Exercise Quality Prediction"
author: "German Blanco"
date: "8 de noviembre de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234567)
library(caret)
library(ggplot2)
```

## Physical Exercise Quality Prediction

This is the Peer-graded Assignment exercise of week 4 (the last week) of Coursera course "Practical Machine Learning". The goal is to predict quality of physical exercise on the Weight Lifting Exercise Dataset referred [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).

### Problem analysis

```{r load_train_csv}
train_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
test_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

A first look at the data reveals more information about the problem that we are trying to solve. In the next step, both the training and test CSV files are analyzed, not in order to shape the prediction algorithm, but in order to know what is it the exercise is about.

### Observations on the exercise objectives:

1 - The training dataset contains series of samples, rather than individual instances. Each series of samples seems to be indentified by a "user_name" and a "sequence_number". All series of samples provided have the same value of "classe", which indicates that it is the series of samples that is classified, instead of the individual instances. There seems to be a chance of aggregating the data of a series of samples in a meaningful way in order to produce a classification of the complete series.

2 - On the other hand, the testing dataset contains only individual instances of the series. There are data points in the training set of the same series as those in the test dataset. It seems that the goal to predict the "classe" of each of this intances in the test set, without any interpretation of what the data is about, may be achieved by looking at the "classe" of the series of the test datapoints in the training set. However, reading what the experiment is about, it seems that the question should be about detecting the quality of the exercise by looking at the data obtained with the sensors. It would seem that an aggregation of the data of each of the series could provide more information to do this, but in the test set only one instance of data point per series is provided, and that seems to indicate that the goal is to identify the quality of the exercise by looking at the sensor data of just one data point of the series.

3 - The test data set contains the same variables as the training dataset, however many of these variables have no valid values at all in the dataset. Since they are not going to be valuable for the prediction of the values of the dataset, they will not be considered for the analysis.

4 - Since the data is grouped in series of measurements, there could be a strong correlation between elements in the same series. Unless proper care is take, the prediction might tend be more associated with associating datapoints with an existing series than with correctly cathegorizing a data point in one of the 6 classes by itself, independently of which series it was included in.


### Cleaning up of data

According to the previous observations:

- There will be no aggregation of data in the series of observations.

- Columns with no valid values in the test set will be filtered out.

- Only sensor data will be considered.

- It is also verified that there are no NA values in the training data.

```{r clean_up_data}
## Verify that all datapoints in a series have the same classe
series_classe <- aggregate(classe ~ user_name + num_window, data=train_csv_df, FUN=function(x) { paste(unique(x)) })
## function that returns the classe for each datapoint and was used to obtain the answers to the quize with 100% accuracy
get_classe <- function(x) { series_classe[series_classe$user_name == x$user_name & series_classe$num_window == x$num_window,] }
## Example "prediction" on the test set
get_classe(test_csv_df[10,])
## Get all columns that have all NA values from test set
cols_all_na <- sapply(test_csv_df, function(x)all(is.na(x)))
## Select only sensor data columns with valid values in test set from training set
train_valid_sensor_data_df <- train_csv_df[,!cols_all_na][,seq(8,59)]
## Add the variable that is to be predicted
train_valid_sensor_data_df$classe <- train_csv_df$classe
## There are no NA values now in the dataframe
are_there_any_nas <- length(train_valid_sensor_data_df$roll_belt) != length(complete.cases(train_valid_sensor_data_df))

```

### Data Slicing

There are now 52 columns of numeric data in the training dataset. The next step is to slice out a subset of our training set in order to verify the prediction algorithm. In order to verify if the prediction algorithm really cathegorizes datapoints by themselves in one of the existing 5 classes, the split will be based on "num_window". This will put only complete series in both the training set and the data set.


```{r slice_out_verify}
## Slice out aprox. 30% of data
inTrain <- createDataPartition(y=series_classe$classe, p=0.7, list=FALSE)
train_valid_sensor_data_df$num_window <- train_csv_df$num_window
my_train_df_and_user_name <- train_valid_sensor_data_df[train_valid_sensor_data_df$num_window %in% inTrain,]
my_verify_df_and_user_name <- train_valid_sensor_data_df[!(train_valid_sensor_data_df$num_window %in% inTrain),]
## Remove the num_window column now
my_train_df <- my_train_df_and_user_name[,-length(colnames(my_train_df_and_user_name))]
my_verify_df <- my_verify_df_and_user_name[,-length(colnames(my_verify_df_and_user_name))]
```

### Normalizing the Data

There are 52 columns of numeric data in the training dataset now. Since it seems that one of the goals would be to understand which of the sensor input data is more relevant, in order to make comparisons easier, normalizing the data seems a good idea.

```{r normalize_data}
## Normalization of data, using all columns except for the last one
preNormalization <- preProcess(my_train_df[,-length(colnames(my_train_df))], method=c("center", "scale"))
my_train_normalized_df <- predict(preNormalization, my_train_df[,-length(colnames(my_train_df))])
```

### Principal Component Analysis

In order to get a first idea of which is the relative importance of variables, with a set of numerical variables, as the one on the table right now, it seems a good idea to apply Principal Component Analysis.

```{r principal_component_analysis}
## Principal Component Analysis using caret
prePCA <- prcomp(my_train_normalized_df)
```

The following figure shows the percentage of variance explained by each of the principla components. The first 20 principal components explain more than 90% of the variance in the dataset.

```{r variance_explained}
std_dev <- prePCA$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex,  type="b")
# PCA using caret and keeping only 20 components
prePCA_caret <- preProcess(my_train_normalized_df, method="pca", pcaComp=20)
trainPC <- predict(prePCA_caret, my_train_normalized_df)
```

### Generalization for Different Users

At this point, since we have six different users in the system, I would like to know if the data under analysis is similar to one user to the other. Even though only datapoints from this six different are present in the test dataset, it seems interesting to know if the prediction should be done in a user per user basis or can be applied to data coming from any user. The following figure shows a clear difference in the data collected from each of the users. This would mean that (at least until data from more users is present) it would make sense to produce predictions per user.

```{r relationship_with_user_name}
train_data <- trainPC
train_data$user_name <- train_csv_df[train_csv_df$num_window %in% inTrain,]$user_name
train_data$classe <- train_csv_df[train_csv_df$num_window %in% inTrain,]$classe
```

```{r plot_1, echo=FALSE}
qplot(user_name, PC1, data=train_data, fill=classe, geom=c("boxplot"))

```

### Poll Ensemble

The goal is to train a classifier per user name. Each classifier will be the result of a poll between a Random Forest, a K Nearest Neighbors and a Naive Bayes classifiers. The prediction will be made by selecting the class that has a majority vote, in case of each classifier voting for a different class, the first vote will be taken.

```{r poll_ensemble}
all_users <- levels(train_csv_df$user_name)
all_methods <- c('rf', 'knn', 'nb')
my_models <- expand.grid(user_name=all_users, method=all_methods)

train_model = function(row, output) {
    row_user_name <- row[1]
    row_method <- row[2]
    row_train_data <- train_data[train_data$user_name == row_user_name, -21]
    train(classe ~ ., data=row_train_data, method=row_method)
}
my_model_list <- apply(my_models, 1, train_model)

tally_votes <- function(row, output) {
    votes <- row[(length(row)-2):length(row)]
    ifelse(length(votes) < 3 | votes[2] != votes[3], votes[1], votes[2])
}

ensemble_predict <- function(input_data_frame) {
    input_data_frame$prediction <- NA
    for (user_name in levels(input_data_frame$user_name)) {
        data_frame = input_data_frame[input_data_frame$user_name == user_name,]
        index = which(my_models$user_name == user_name & my_models$method == 'rf')
        model = my_model_list[index]
        data_frame$predict_rf <- predict(model, data_frame)[[1]]
        index = which(my_models$user_name == user_name & my_models$method == 'knn')
        model = my_model_list[index]
        data_frame$predict_knn <- predict(model, data_frame)[[1]]
        index = which(my_models$user_name == user_name & my_models$method == 'nb')
        model = my_model_list[index]
        data_frame$predict_nb <- predict(model, data_frame)[[1]]
    }
    input_data_frame$prediction <- apply(input_data_frame, 1, tally_votes)
    input_data_frame
}

```

### Results

The accuracy of the ensemble classifier in the verification dataset is shown in the following confusion matrix.

```{r estimated_accuracy}
my_verify_normalized_df <- predict(preNormalization, my_verify_df[,-length(colnames(my_verify_df))])
verifyPC <- predict(prePCA_caret, my_verify_normalized_df)
verify_data <- verifyPC
verify_data$user_name <- train_csv_df[!(train_csv_df$num_window %in% inTrain),]$user_name
verify_data <- ensemble_predict(verify_data)
classe <- train_csv_df[!(train_csv_df$num_window %in% inTrain),]$classe
confusionMatrix(verify_data$prediction, classe)

```

It is even possible to check the predictions in the test dataset, since there is a function that obtains the class of the corresponding data series.

```{r accuracy_on_test_set}
test_valid_sensor_data_df <- test_csv_df[,!cols_all_na][,seq(8,59)]
my_test_normalized_df <- predict(preNormalization, my_test_df[,-length(colnames(my_test_df))])
testPC <- predict(prePCA_caret, my_test_normalized_df)
test_data <- testPC
test_data$user_name <- test_csv_df$user_name
test_data <- ensemble_predict(test_data)
classe <- apply(test_data, 1, get_classe)
confusionMatrix(test_data$prediction, classe)
```
