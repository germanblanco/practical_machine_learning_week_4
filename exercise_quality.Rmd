---
title: "Physical Exercise Quality Prediction"
author: "German Blanco"
date: "8 de noviembre de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234567)
library(caret)
library(ggplot2)
```

## Physical Exercise Quality Prediction

This is the Peer-graded Assignment exercise of week 4 (the last week) of Coursera course "Practical Machine Learning". The goal is to predict quality of physical exercise on the Weight Lifting Exercise Dataset described in [this page](http:/groupware.les.inf.puc-rio.br/har).

### Problem analysis

```{r load_train_csv}
train_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
test_csv_df <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```

A first look at the data reveals more information about the problem that we are trying to solve. In the next step, both the training and test CSV files are analyzed, not in order to shape the prediction algorithm, but in order to know what is it the exercise is about.

### Observations on the exercise objectives:

1 - The training dataset contains series of samples, rather than individual instances. Each series of samples seems to be identified by a "user_name" and a "sequence_number". All series of samples provided have the same value of "classe", which indicates that it is the series of samples that is classified, instead of the individual instances. There seems to be an opportunity of aggregating the data of a series of samples in a meaningful way in order to produce a classification of the complete series.

2 - On the other hand, the testing dataset contains only individual instances of the series. There are data points in the training set of the same series as those in the test dataset. It seems that the goal to predict the "classe" of each of this intances in the test set, without any interpretation of what the data is about, may be achieved by looking at the "classe" of the series of the test datapoints in the training set. However, reading what the experiment is about, it seems that the question should be about detecting the quality of the exercise by looking at the data obtained with the sensors. It would seem that an aggregation of the data of each of the series could provide more information to do this, but in the test set only one instance of data point per series is provided, and that seems to indicate that the goal is to identify the quality of the exercise by looking at the sensor data of just one data point of the series.

3 - The test data set contains the same variables as the training dataset, however many of these variables have no valid values at all in the dataset. Since they are not going to be valuable for the prediction of the values of the dataset, they will not be considered for the analysis.

4 - Since the data is grouped in series of measurements, there could be a strong correlation between elements in the same series. Unless proper care is taken, the prediction might tend to be more about associating datapoints with an existing series than with correctly cathegorizing a data point in one of the 6 classes by itself, independently of which series it was included in.


### Cleaning up of data

According to the previous observations:

- There will be no aggregation of data in the series of observations.

- Columns with no valid values in the test set will be filtered out.

- Only sensor data will be considered.

- Complete series will be used in training and test dataset.

- It is also verified that there are no NA values in the training data.

```{r clean_up_data}
## Verify that all datapoints in a series have the same classe
series_classe <- aggregate(classe ~ user_name + num_window, data=train_csv_df, FUN=function(x) { paste(unique(x)) })
## function that returns the classe for each datapoint and was used to obtain the answers to the quize with 100% accuracy
get_classe <- function(x) { series_classe[series_classe$user_name == x$user_name & series_classe$num_window == x$num_window,] }
## Example "prediction" on the test set
get_classe(test_csv_df[10,])
## Get all columns that have all NA values from test set
cols_all_na <- sapply(test_csv_df, function(x)all(is.na(x)))
## Select only sensor data columns with valid values in test set from training set
train_valid_sensor_data_df <- train_csv_df[,!cols_all_na][,seq(8,59)]
## Add the variable that is to be predicted
train_valid_sensor_data_df$classe <- train_csv_df$classe
## There are no NA values now in the dataframe
are_there_any_nas <- length(train_valid_sensor_data_df$roll_belt) != length(complete.cases(train_valid_sensor_data_df))
```

### Data Slicing

There are now 52 columns of numeric data in the training dataset. The next step is to slice out a subset of our training set in order to verify the prediction algorithm. In order to verify if the prediction algorithm really cathegorizes datapoints by themselves in one of the existing 5 classes, the split will be based on "num_window". This will put only complete series in both the training set and the data set. There will be two splits in order to combine different algorithms in an ensemble later.`

```{r slice_out_verify}
## Slice out aprox. 25% of data of the total training dataset
inTrainAll <- createDataPartition(y=series_classe$classe, p=0.75, list=FALSE)
## Slice out aprox. 25% of data again to set aside for training the ensemble
inTrain <- createDataPartition(y=series_classe[inTrainAll,]$classe, p=0.75, list=FALSE)
## Recover series identification information
train_valid_sensor_data_df$num_window <- train_csv_df$num_window
## Data for final verification of the ensemble
inVerify <- train_valid_sensor_data_df$num_window %in% series_classe[-inTrainAll,]$num_window
## Data for training the models
inTrainModels <- !inVerify & (train_valid_sensor_data_df$num_window %in% series_classe[inTrain,]$num_window)
## Data for training the ensemble of models
inTrainEnsemble <- !inVerify & !inTrainModels
## Datasets for each of the 3 partitions
my_verify_df_with_num_window <- train_valid_sensor_data_df[inVerify,]
my_train_ensemble_df_with_num_window <- train_valid_sensor_data_df[inTrainEnsemble,]
my_train_df_with_num_window <- train_valid_sensor_data_df[inTrainModels,]
## Remove the num_window column now
my_train_df <- my_train_df_with_num_window[,-length(colnames(my_train_df_with_num_window))]
my_train_ensemble_df <- my_train_ensemble_df_with_num_window[,-length(colnames(my_train_df_with_num_window))]
my_verify_df <- my_verify_df_with_num_window[,-length(colnames(my_verify_df_with_num_window))]
```

### Preprocessing the Data

There are 52 columns of numeric data in the training dataset now. In order to help convergence of algorithms, and in order to be able to compare the relevance of parameters, normalization and PCA will be applied.

```{r preprocessing_data}
## Preprocessing of data, using all columns except for the last one
preNormalization <- preProcess(my_train_df[,-length(colnames(my_train_df))], method=c("center", "scale"))
my_train_normalized_df <- predict(preNormalization, my_train_df[,-length(colnames(my_train_df))])
## Principal Component Analysis using caret
prePCA <- prcomp(my_train_normalized_df)
```

The following figure shows the percentage of variance explained by each of the principal components. The first 20 principal components explain more than 90% of the variance in the dataset, so there is a lot of redundancy.

```{r variance_explained}
std_dev <- prePCA$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex,  type="b")
# PCA using caret and keeping only 20 components
prePCA_caret <- preProcess(my_train_normalized_df, method="pca", pcaComp=20)
trainPC <- predict(prePCA_caret, my_train_normalized_df)
```

### Generalization for Different Users

At this point, since we have six different users in the system, I would like to know if the data under analysis is similar from one user to the other. Even though only datapoints from this six different are present in the test dataset, it seems interesting to know if the prediction should be done in a user per user basis or can be applied to data coming from any user. The following figure shows a clear difference in the data collected from each of the users. This would mean that (at least until data from more users is present) it would make sense to produce predictions per user. "user_name" will be added to the Principal Components as input to the models, and it will not be reliable to predict the "classe" for datapoints that have a "user_name" that was not included in the training set. Fortunately, this does not occur in our current test data.

```{r relationship_with_user_name}
train_data_rf <- my_train_df
train_data_rf$user_name <- train_csv_df[inTrainModels,]$user_name
train_data_rf$classe <- train_csv_df[inTrainModels,]$classe
dummy <- dummyVars(~ user_name, train_csv_df[inTrainModels,])
train_data_knn <- cbind(trainPC, 10 * predict(dummy, train_csv_df[inTrainModels,]))
train_data_knn$classe <- train_csv_df[inTrainModels,]$classe

```

```{r plot_1, echo=FALSE}
qplot(train_data_rf$user_name, train_data_knn$PC1, fill=train_data_knn$classe, geom=c("boxplot"))
```

### Poll Ensemble

The goal is to train a ensemble of a Random Forest, a K Nearest Neighbors and a Naive Bayes classifiers.

```{r poll_ensemble}
## Since these are computationally expensive, set up parallel processing
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
## Training of the three models
nb_model <- train(classe ~ ., data=train_data, method='nb')
## Predictions on the training set for the ensemble
train_ensemble_data_rf <- my_train_ensemble_df
train_ensemble_data_rf$user_name <- train_csv_df[inTrainEnsemble,]$user_name
train_ensemble_data_rf$classe <- train_csv_df[inTrainEnsemble,]$classe
rf_model <- train(classe ~ ., data=train_data_rf, method='rf')
prePCA_ensemble <- prcomp(my_train_normalized_df)
my_train_ensemble_normalized_df <- predict(preNormalization, my_train_ensemble_df[,-length(colnames(my_train_ensemble_df))])
trainEnsemblePC <- predict(prePCA_caret, my_train_ensemble_normalized_df)
train_ensemble_data_knn <- cbind(trainEnsemblePC, 10 * predict(dummy, train_csv_df[inTrainEnsemble,]))
train_ensemble_data_knn$classe <- train_csv_df[inTrainEnsemble,]$classe
knn_model <- train(classe ~ ., data=train_data_knn, method='knn')
nb_model <- train(classe ~ ., data=train_data_knn, method='nb')
nb_model <- train(classe ~ ., data=train_data_rf, method='nb')
train_ensemble_data <- data.frame(train_csv_df[inTrainEnsemble,]$classe)
train_ensemble_data$prediction_rf <- predict(rf_model, train_ensemble_data_rf)
train_ensemble_data$prediction_knn <- predict(knn_model, train_ensemble_data_knn)
train_ensemble_data$prediction_nb <- predict(nb_model, train_ensemble_data)
## Measure accuracy for each of the models
confusionMatrix(train_ensemble_data$prediction_rf, train_ensemble_data$classe)
confusionMatrix(train_ensemble_data$prediction_knn, train_ensemble_data$classe)
confusionMatrix(train_ensemble_data$prediction_nb, train_ensemble_data$classe)
## Build a new model that combines the predictions
predDF <- data.frame(
  train_ensemble_data$prediction_rf,
  train_ensemble_data$prediction_knn,
  train_ensemble_data$prediction_nb,
  classe=train_ensemble_data$classe)
classe_levels <- levels(train_ensemble_data$classe)
train_ensemble_data$prediction_ensemble_str <- ifelse(
  train_ensemble_data$prediction_nb == train_ensemble_data$prediction_rf,
  classe_levels[train_ensemble_data$prediction_nb],
  classe_levels[train_ensemble_data$prediction_knn])
train_ensemble_data$prediction_ensemble <- as.factor(train_ensemble_data$prediction_ensemble_str)
## Measure accuracy for the ensemble
confusionMatrix(train_ensemble_data$prediction_ensemble, train_ensemble_data$classe)

all_users <- levels(train_csv_df$user_name)
all_methods <- c('rf', 'knn', 'nb')
my_models <- expand.grid(user_name=all_users, method=all_methods)

train_model = function(row, output) {
    row_user_name <- row[1]
    row_method <- row[2]
    row_train_data <- train_data[train_data$user_name == row_user_name, -21]
    train(classe ~ ., data=row_train_data, method=row_method)
}
my_model_list <- apply(my_models, 1, train_model)

tally_votes <- function(row, output) {
    votes <- row[(length(row)-2):length(row)]
    ifelse(length(votes) < 3 | votes[2] != votes[3], votes[1], votes[2])
}

ensemble_predict <- function(input_data_frame) {
    input_data_frame$prediction <- NA
    for (user_name in levels(input_data_frame$user_name)) {
        data_frame = input_data_frame[input_data_frame$user_name == user_name,]
        index = which(my_models$user_name == user_name & my_models$method == 'rf')
        model = my_model_list[index]
        data_frame$predict_rf <- predict(model, data_frame)[[1]]
        index = which(my_models$user_name == user_name & my_models$method == 'knn')
        model = my_model_list[index]
        data_frame$predict_knn <- predict(model, data_frame)[[1]]
        index = which(my_models$user_name == user_name & my_models$method == 'nb')
        model = my_model_list[index]
        data_frame$predict_nb <- predict(model, data_frame)[[1]]
    }
    input_data_frame$prediction <- apply(input_data_frame, 1, tally_votes)
    input_data_frame
}

```

### Results

The accuracy of the ensemble classifier in the verification dataset is shown in the following confusion matrix.

```{r estimated_accuracy}
my_verify_normalized_df <- predict(preNormalization, my_verify_df[,-length(colnames(my_verify_df))])
verifyPC <- predict(prePCA_caret, my_verify_normalized_df)
verify_data <- verifyPC
verify_data$user_name <- train_csv_df[!(train_csv_df$num_window %in% inTrain),]$user_name
verify_data <- ensemble_predict(verify_data)
classe <- train_csv_df[!(train_csv_df$num_window %in% inTrain),]$classe
confusionMatrix(verify_data$prediction, classe)

```

It is even possible to check the predictions in the test dataset, since there is a function that obtains the class of the corresponding data series.

```{r accuracy_on_test_set}
test_valid_sensor_data_df <- test_csv_df[,!cols_all_na][,seq(8,59)]
my_test_normalized_df <- predict(preNormalization, my_test_df[,-length(colnames(my_test_df))])
testPC <- predict(prePCA_caret, my_test_normalized_df)
test_data <- testPC
test_data$user_name <- test_csv_df$user_name
test_data <- ensemble_predict(test_data)
classe <- apply(test_data, 1, get_classe)
confusionMatrix(test_data$prediction, classe)
```
